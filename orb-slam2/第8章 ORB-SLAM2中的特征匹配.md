## 单目初始化中的特征匹配
在单目初始化的时候，由于没有任何的先验信息，如何进行特征匹配呢？
如果采用暴力匹配，那会导致匹配的效率极低且效果不好。

在ORB-SLAM2中，参与初始化的两帧是比较接近的，也就是现在第1帧中提取特征点的坐标，保持这个坐标不变，在第2帧中再按照这个坐标为圆心，以一定的半径画圆（源码中使用的是正方形），对应的匹配点也应该在这个圆中。
如图所示，假设第1帧中提取到的某个特征点（$M_1$）的坐标为$P=(x_1,y_1)$，在第2帧中的相同坐标（圆心$O=(x_1,y_1)$）附近画一个半径为$r$的圆，圆内的所有点都是候选的匹配特征点。再用$M_1$和所有的候选匹配点进行匹配，最终找到特征点$M=(x_2,y_2)$就完成了特征匹配。
当然这个特征点$M=(x_2,y_2)$还应该满足一些条件：
- 条件1：遍历候选匹配点中最优和次优的匹配，最优匹配对应的描述子距离比次优匹配对应的描述子距离小于设定的比例。
- 条件2：最优匹配对应的描述子距离小于设定的阈值。
- 条件3：经过方向一致性检验。

![](https://cdn.jsdelivr.net/gh/liu-moon/pic@main/img/20231005155505.png)


### 如何快速确定候选匹配点
因为在系统的初始化过程中，无法知道相机的运动大小，所以在确定候选匹配点的时候，搜索圆半径会设置得相对比较大，代码中半径为100个像素，这样圆内的区域也太多了，如果对每个特征点逐像素匹配，代价非常大，在ORB-SLAM2中使用了划分网格的方式进行了加速。提取完特征点之后会直接将特征点划分到不同的网格中并记录在mGrid里面，在搜索的时候是以网格为单位进行的，代码中的网格尺寸默认为64像素x48像素，这样圆内包含的网格数目相比像素数目就大大减少了。具体的过程如下。
1. 现根据圆的范围确定圆的上、下、左、右边界分别在哪个网格内。图8-1中圆的边界在水平坐标轴上的坐标范围是mMinCellX~mMaxCellX，在垂直坐标轴上的坐标范围是mMinCellY~mMaxCellY。mnMinX是图像的左边界对应的像素坐标。
2. 遍历圆形区域内所有的网格。如果某个网格内没有特征点，那么直接跳过；如果某个网格内有特征点（图中绿色方框），则遍历这些特征点，判断这些特征点是否符合要求的金字塔层级，是否在圆内，如果满足条件，则会把特征点作为候选特征点。这种方式大大提高了搜索效率。

这里我比较迷惑，网格法如何加速的？

### 方向一致性检验
经过上面两个条件的检验，还需要进行方向一致性检验。因为通过特征点匹配后的结果仍然不一定准确，所以需要剔除其中的错误匹配。原理是统计两张图像所有匹配对中两个特征点主方向的差，构建一个直方图。由于两张图像整体发生了运动，因此特征点匹配对主方向整体会有一个固定一致的变化。通常直方图中前三个最大的格子（代码中bin）里就是正常的匹配点对，那些误匹配的特征点对此时就会暴露出来，落在直方图之外的其他格子里面，这些就是需要剔除的错误匹配。


## 8.2 通过词袋进行特征匹配

### 8.2.1 什么是词袋
词袋（Bag of Words，BoW）最早在自然语言处理领域应用，其中的Words就表示文本中的单词。后来研究者把词袋用在视觉SLAM领域，这时Words表示的是图像中的局部信息，如特征点。
注意，在词袋中的单词是没有顺序的，也就是我们之关系某张图像中有没有出现某个单词，出现了多少次，而不关系到底是在图像哪个位置出现的，也不关心单词出现的先后顺序，这样就大大简化了词袋模型的表达方式，节省了存储空间，可以实现高效索引。

### 8.2.2 词袋有什么用
在ORB-SLAM2中，词袋主要用于两个方面。
1. 用于加速特征匹配。在没有任何先验信息的情况下，如果想要对两张图像中提取的特征点进行匹配，则通常只能用暴力匹配的方法，这样不仅非常慢，而且很容易出现错误匹配。而通过词袋搜索匹配，只需要比较同一个节点下的特征点，因为同一个节点下的特征点通常都是比较相似的，这相当于提前对特征相似的特征点进行了区域划分，不仅提高了搜索效率，也能减少很多错误匹配
2. 用于闭环检测。闭环检测的核心就是判断两张图像是不是同一个场景，也就是判断图像的相似性。判断两张图像的相似性对于人类来说非常容易，但对于计算机来说是相对困难的。比如两张图像很可能会有较大的视角变化，也可能会在不同的时间、不同的光照条件下拍摄同一个场景，此时两张图像会有较大差异。即使是同一时间、用同样的视角拍摄，由于相机本身曝光等参数不同，拍摄的两张图像也会有较大差异。
以上种种复杂的情况下，很难找到一种既简单又通用的传统办法来判读两张图像是否相似。
而词袋可以解决上述问题。因为词袋用图像特征的集合作为单词，只关心图像中这些单词出现的频率，不关系单词出现的位置，而且通常采用的是二进制图像特征，对光照变化比较鲁棒，使用词袋进行闭环检测更符合人类的感知方式。
目前在主流的SLAM开源算法中，用词袋进行闭环检测是最主要的方法。其中DBoW2是使用词袋的一个第三方库。
词袋模型的缺点如下：
首先需要离线训练字典树（Vocabulary Tree），也称为字典。我们也可以使用别人训练好的字典。
其次，系统启动时需要先加载字典，而这个字典一般比较大（可能有100多兆），加载会慢一点，也会占用内存空间。不过现在有很多种方法可以将字典压缩到几兆字节，有效提升了加载速度，减少了内存占用。
最后，每帧图像中的特征点需要先通过这个离线字典在线转换成特征向量（FeatureVector）和词袋向量（BowVector）

### 8.2.4 离线训练字典
1. 准备好足够数量的图像数据集。数据集最好涵盖不同光照、不同场景、不同天气和不同季节等条件下拍摄的图像集合，种类尽量多而不重复，比如在ORB-SLAM2中使用的字典训练数据集包括几万张图片。这样做是为了尽可能多地涵盖不同的情况，使得ORB-SLAM2在各种情况下词袋都能工作。当然，如果只在某种特定场景中使用，则可以只采集该场景中尽可能不同类型的图像。
2. 遍历以上所有的训练图像，对每张图像提取ORB特征点。最后得到的特征点总数目是非常大的，比如ORB-SLAM2使用的离线字典就有超过108万个特征点。
3. 建立字典树。将提取到的所有图像特征点的描述子用K-means聚类，变成K个集合，作为字典树的第1层级，然后对每个集合内部重复聚类操作，就得到了字典树的第2层级，层级每个集合内部重复上述聚类操作，最后得到深度为L，分支数为K的字典树。如图8-8所示，第0层是根节点，离根节点最远的一层是叶子，也成为单词（Word）。
4. 根据每个单词在训练集中出现的频率给其赋予一定的权重，其在训练集中出现的次数越多，说明辨别力越差，赋予的权重就越低。


![](https://cdn.jsdelivr.net/gh/liu-moon/pic@main/img/20231023150949.png)

通常通过第三方库DBoW2或更新的版本DBoW3训练数据集生成字典。利用DBoW2库中的函数可以很方便地把训练好的字典保存为.txt文件，这个字典文件是通用的，也可以借别人训练好的字典来用。

### 8.2.5 在线生成词袋向量
以上是离线训练字典的过程。在ORB-SLAM2中，对于新的一帧图像，会利用上面的离线字典为当前图像在线生成词袋向量，具体流程如下。
1. 对新的一帧图像先提取ORB特征点，特征点描述子和离线字典中的一致。
2. 对于每个特征点的描述子，从离线创建好的字典树中自上而下开始寻找自己的位置，从根节点开始，用该描述子和每个节点的描述子计算汉明距离，选择汉明距离最小的节点作为自己所在的节点，一直遍历到叶子节点。最终把叶子的单词id和权重等属性赋予这个特征点。在图8-8中，紫色的线表示一个特征点从根节点到叶子节点的搜索过程。在树状结构中，这个过程是非常快的。
图8-8中的level up是什么意思？
可以简单地将level up理解为搜索范围。每个描述子转化为单词后会包含一个属性，叫做单词所属的节点ID，这个节点ID距离叶子的层级就是level up。在进行特征匹配时，只在该节点所属的节点ID内部搜索即可。如果level up设置得比较大，单词所属的节点ID会比较靠近根节点，那么搜索范围就会扩大，极端情况是在整个字典树中进行搜索，肯定相当慢；如果level up设置得比较小，单词所属的节点ID会比较靠近叶子节点，那么很可能搜不到匹配的特征点。因此level up要设置为一个合适的值，在ORB-SLAM2中，通常设置level up=3。

将一张图像中所有的特征点按照上述方法，通过字典树最终转化为两个向量——BowVector和FeatureVector。

这些操作相当于对当前图像信息进行了压缩，这两个向量对特征点快速匹配、闭环检测、重定位的意义重大。

先说BowVector，它的数据结构如下。

```cpp
std::map<WordId, WordValue>
```

其中，WordId和WordValue表示单词Word在所有叶子节点中距离最近叶子节点的ID和权重，这和我们前面介绍的一致。对于同一个单词ID，它的权重是累加并不断更新的。

再来介绍FeatureVector，它的数据结构如下。

```cpp
std::map<NodeId,std::vector<unsigned int>>
```

其中，NodeId并不是该叶子节点的直接父节点ID，而是距离叶子节点深度为level up的节点的ID，这在前面也反复提到了。在进行特征匹配时，搜索该单词的匹配点时，搜索范围是和它具有同样NodeId的所有子节点，搜索区域间图8-8中单词的搜索范围。所以，搜索范围的大小是根据level up确定的，level up值越大，搜索范围越广，搜索速度越慢；level up值越小，搜索范围越小，搜索速度越快，但能够匹配的特征点就越少。

第2个参数std::vector<unsigned int>中实际存储的是NodeId下所有特征点在图像中的索引。


### 8.2.6源码解析

下面来看在ORB-SLAM2特征匹配中词袋具体是如何加速特征点匹配的。前面说过，FeatureVector的第一个元素就是节点的ID，第二个元素是一个向量，存储的是该节点内所有的特征点在图像中的索引。在搜索特征点时，只需要找到在相同节点ID内的两张图像的特征点，挨个匹配即可。
下面总结词袋匹配方法的优缺点。

- 优点1：因为只需要在同一个节点下搜索候选匹配点，不需要地图点投影，所以匹配效率很高。
- 优点2：不需要位姿即可匹配，比地图点投影匹配方法的应用场景更广泛，比如可以用于跟踪丢失重定位、闭环检测等场景。
- 缺点：比较依赖字典，能够成功匹配到的特征对较少，适用于粗糙的特征匹配来估计初始位姿。

## 8.3 通过地图点投影进行特征匹配
ORB-SLAM2中用得最多的匹配方式就是投影匹配，不同的参数有多个重载函数，不过它们的基本思想都差不多，我们后面会以最复杂的一个函数为例进行说明。

### 8.3.1 投影匹配原理
“投影”投的是地图点，这些地图点的来源主要如下。
- 在恒速模型跟踪中，投影的地图点来自前一个普通帧。
- 在局部地图模型中，投影的地图点来自所有局部地图点。
- 在重定位跟踪中，投影的地图点来自候选关键帧。
- 在闭环线程中，投影的地图点来自闭环关键帧及其共视关键帧。

其次，怎么投影？答案是通过位姿来投影。比如在恒速模型跟踪中，在位姿还没有估计出来时，可以假定一个初始位姿来投影，即使位姿不准确也没关系，因为是在投影位置附近区域进行特征匹配的，而且后续还会持续优化位姿；如果已经有估计的位姿，就可以用这个位姿直接投影。

最后，投影后如何匹配？地图点经过位姿变换后，对应当前普通帧或关键帧的相机坐标系下的三维点，然后用针孔模型投影到图像平面的二维图像坐标上，再在该坐标周围的圆形区域内寻找候选匹配特征点，这和前面快速确定候选匹配特征点用的是同样的函数。

图8-9所示是恒速模型跟踪中投影匹配的一个示意图。图8-9(a)所示为上一个成功跟踪的普通帧，其中绿色的点为提取的特征点，红色的点为特征点对应的地图点。图8-9(b)所示为当前普通帧，将上一帧中的地图点投影到当前帧中，在投影点附近的圆形区域（图8-9(b)中红色的圆）内搜索候选匹配点。

![](https://cdn.jsdelivr.net/gh/liu-moon/pic@main/img/20231023160035.png)

### 8.3.2 根据相机运动方向确定金字塔搜索层级
恒速模型跟踪中的特征匹配相对复杂，它会根据相机前进或后退（相对于光轴方向）来选择不同层级的金字塔中的特征点来作为候选匹配点，保持特征点能够正确匹配。

第一步，判断相机是否有明显的前进或后退。这就需要求解当前帧到上一帧的平移变化量。记$T_{cw}$，$T_{lw}$分别是当前帧、上一帧的位姿，$R$，$t$分别是旋转矩阵和平移向量。写成矩阵的形式：
$$\begin{array}{l}
\boldsymbol{T}_{\mathrm{cw}}=\left[\begin{array}{cc}
\boldsymbol{R}_{\mathrm{cw}} & \boldsymbol{t}_{\mathrm{cw}} \\
\boldsymbol{O} & 1
\end{array}\right] \\
\end{array}$$

$$\begin{array}{l}
\boldsymbol{T}_{\mathrm{cw}}^{-1}=\left[\begin{array}{cc}
\boldsymbol{R}_{\mathrm{cw}}^{\top} & -\boldsymbol{R}_{\mathrm{cw}}^{\top} \boldsymbol{t}_{\mathrm{cw}} \\
\boldsymbol{O} & 1
\end{array}\right] \\
\end{array}$$

$$\begin{array}{l}
\boldsymbol{T}_{\mathrm{lw}}=\left[\begin{array}{cc}
\boldsymbol{R}_{\mathrm{lw}} & \boldsymbol{t}_{\mathrm{lw}} \\
\boldsymbol{O} & 1
\end{array}\right]
\end{array}$$

那么，当前帧到上一帧的位姿变换$T_{lc}$为
$$\begin{aligned}
\boldsymbol{T}_{\mathrm{lc}} & =\boldsymbol{T}_{\mathrm{lw}} \boldsymbol{T}_{\mathrm{cw}}^{-1} \\
& =\left[\begin{array}{cc}
\boldsymbol{R}_{\mathrm{lw}} & \boldsymbol{t}_{\mathrm{lw}} \\
\boldsymbol{O} & 1
\end{array}\right]\left[\begin{array}{cc}
\boldsymbol{R}_{\mathrm{cw}}^{\top} & -\boldsymbol{R}_{\mathrm{cw}}^{\top} \boldsymbol{t}_{\mathrm{cw}} \\
\boldsymbol{O} & 1
\end{array}\right] \\
& =\left[\begin{array}{cc}
\boldsymbol{R}_{\mathrm{lw}} R_{\mathrm{cw}}^{\top} & -\boldsymbol{R}_{\mathrm{lw}} \boldsymbol{R}_{\mathrm{cw}}^{\top} \boldsymbol{t}_{\mathrm{cw}}+\boldsymbol{t}_{\mathrm{lw}} \\
\boldsymbol{O} & 1
\end{array}\right]
\end{aligned}$$
当前帧到上一帧的平移向量为
$$\boldsymbol{t}_{\mathrm{lc}}=-\boldsymbol{R}_{\mathrm{lw}} \boldsymbol{R}_{\mathrm{cw}}^{\top} \boldsymbol{t}_{\mathrm{cw}}+\boldsymbol{t}_{\mathrm{lw}}$$
我们根据当前帧到上一帧的平移向量$t_{lc}$在z轴的分量和相机基线距离比较来判断是否发生了明显的前进或后退。
第2步，根据第1步中判断的相机运动方向（前进或后退），确定搜索候选匹配特征点的尺度范围。
图8-10（b）所示为上一帧相机原始位置，如果当前帧明显沿z轴方向向前移动（见图8-10（a）），那么根据“近大远小”的规则，想要和8-10（b）中level=0的特征点匹配，就需要在8-10（a）中金字塔的更高层级level=1上搜索才能正确匹配。同样，如果当前帧明显沿z轴方向向后移动（见图8-10（c）），那么想要和图8-10（b）中level=1的特征点匹配，就需要在图8-10（c）中金字塔的最低层级level=0上搜索才能正确匹配。
为什么确定相机前进或后退要和相机的基线进行比较？单目相机没有基线怎么办？
这里相机前进或后退的判定只针对双目相机和RGB-D相机，因为它们有绝对尺度，而且有已知的基线可以进行比较。至于为什么用相机基线作为判定标准，这可能是为了方便度量而取的经验值。对于单目相机，因为没有办法获得绝对尺度，所以不进行前进或后退的判定，在搜索匹配时限定在当前金字塔层级±1的范围内即可。
![](https://cdn.jsdelivr.net/gh/liu-moon/pic@main/img/20231023163645.png)


## 8.4 通过Sim(3)变换进行相互投影匹配
### 8.4.1 相互投影匹配原理
在闭环线程中，闭环候选帧和当前关键帧最早是通过词袋进行搜索匹配的，你知道为什么吗？
因为它们间隔的时间比较远，没有先验信息，这时用词袋搜索是最合适的。
但是用词袋搜索有一个缺点，就是会有漏匹配。而成功的闭环需要在闭环候选帧和当前帧之间尽可能多地建立更多的匹配关系，这时可以利用初步估计的Sim(3)位姿进行相互投影匹配，忽略已经匹配的特征点，只在尚未匹配的特征点中挖掘新的匹配关系。
什么是相互投影匹配呢？
假设待匹配的关键帧分别是KF1、KF2，以图8-11为例介绍具体方法。

![](https://cdn.jsdelivr.net/gh/liu-moon/pic@main/img/20231023170139.png)

第1步，先统计它们之间已经匹配好的特征点对（图8-11中黑色连线表示已经匹配好的特征点对），目的是在后续投影中跳过这些已经匹配好的特征点对，从剩下的未匹配的特征点中寻找新的匹配关系。
第2步，把KF1的地图点用Sim21变换投影到KF2图像上，在投影点附近一定范围（图8-11中右侧蓝色的圆圈）内寻找候选匹配点，从中选择描述子距离最近的点作为最佳匹配点。
第3步，把KF2的地图点用Sim12变换投影到KF1图像上，在投影点附近一定的范围（图8-11中左侧紫色的圆圈）内寻找候选匹配点，从中选择描述子距离最近的点作为最佳匹配点。
第4步，找出同时满足第2、3步要求的特征点匹配对，也就是在两次相互匹配中同时出现的匹配点对，作为最终可靠的新的匹配结果（图8-11中红色连线对应的匹配点）。

这里的Sim(3)是什么呢？
Sim(3)中的“Sim”是Similarity的缩写，“3”表示三维空间。关于如何求解Sim(3)我们后面会详细介绍，这里我们把Sim(3)当作带尺度因子的变换矩阵即可。相似变换Sim(3)矩阵记为
$$\begin{array}{c}
S=\left[\begin{array}{cc}
s R & t \\
O & 1
\end{array}\right] 
\end{array}$$

Sim(3)变换矩阵的逆为

$$\begin{array}{c}
S^{-1}=\left[\begin{array}{cc}
\frac{1}{s} R^{\top} & -\frac{1}{s} R^{\top} t \\
O & 1
\end{array}\right]
\end{array}$$

它们在我们下面的源码中会用到。

### 8.4.2 源码解析
下面的代码是在闭环检测线程中，用Sim(3)变换对当前关键帧和候选闭环关键帧进行相互投影匹配，目的是生成更多的匹配点对。

